{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MEt3R with MASt3R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data1/usman/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "/data1/usman/micromamba/env/envs/met3r/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /data1/usman/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from met3r import MEt3R\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Initialize MEt3R\n",
    "metric = MEt3R(\n",
    "    img_size=IMG_SIZE, # Default, set to `None` to use the input resolution on the fly!\n",
    "    use_norm=True, # Default \n",
    "    backbone=\"mast3r\", # Default, select from [\"mast3r\", \"dust3r\", \"raft\"]\n",
    "    feature_backbone=\"dino16\", # Default, select from [\"dino16\", \"dinov2\", \"maskclip\", \"vit\", \"clip\", \"resnet50\"]\n",
    "    feature_backbone_weights=\"mhamilton723/FeatUp\", # Default\n",
    "    upsampler='featup', # Default, select from [\"featup\", \"nearest\", \"bilinear\", \"bicubic\"]\n",
    "    distance=\"cosine\", # Default, [\"cosine\", \"lpips\", \"rmse\", \"psnr\", \"mse\", \"ssim\"]\n",
    "    freeze=True, # Default\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 2, 3, 256, 256])\n",
      "Score:  tensor([0.0007, 0.0008, 0.0007, 0.0055, 0.0007, 0.0008, 0.0008, 0.0007, 0.0037,\n",
      "        0.0007], device='cuda:0')\n",
      "0.0015098018338903785\n",
      "Used memory: 2742.91 MB\n",
      "Cached memory: 2872.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs of shape (batch, views, channels, height, width): views must be 2\n",
    "# RGB range must be in [-1, 1]\n",
    "# Reduce the batch size in case of CUDA OOM\n",
    "# inputs = torch.randn((10, 2, 3, IMG_SIZE, IMG_SIZE)).cuda()\n",
    "inputs = torch.randn((10,1, 3, IMG_SIZE, IMG_SIZE)).cuda()\n",
    "inputs = inputs.repeat(1, 2, 1, 1, 1)  # Repeat to create 2 views\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "inputs = inputs.clip(-1, 1)\n",
    "\n",
    "# Evaluate MEt3R\n",
    "score, *_ = metric(\n",
    "    images=inputs, \n",
    "    return_overlap_mask=False, # Default \n",
    "    return_score_map=False, # Default \n",
    "    return_projections=False # Default \n",
    ")\n",
    "\n",
    "# Should be between 0.22 - 0.29\n",
    "print(f\"Score: \", score)\n",
    "print(score.mean().item())\n",
    "\n",
    "# Clear up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "used_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # in MB\n",
    "cached_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # in MB\n",
    "\n",
    "print(f\"Used memory: {used_memory:.2f} MB\")\n",
    "print(f\"Cached memory: {cached_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MEt3R with DUSt3R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data1/usman/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /data1/usman/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from met3r import MEt3R\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Initialize MEt3R\n",
    "metric = MEt3R(\n",
    "    img_size=IMG_SIZE,\n",
    "    use_norm=True,\n",
    "    backbone=\"dust3r\",\n",
    "    feature_backbone=\"dino16\",\n",
    "    feature_backbone_weights=\"mhamilton723/FeatUp\",\n",
    "    upsampler=\"featup\",\n",
    "    distance=\"cosine\",\n",
    "    freeze=True, \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3464803993701935\n",
      "Used memory: 2297.43 MB\n",
      "Cached memory: 2626.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs of shape (batch, views, channels, height, width): views must be 2\n",
    "# RGB range must be in [-1, 1]\n",
    "# Reduce the batch size in case of CUDA OOM\n",
    "inputs = torch.randn((10, 2, 3, IMG_SIZE, IMG_SIZE)).cuda()\n",
    "inputs = inputs.clip(-1, 1)\n",
    "\n",
    "# Evaluate MEt3R\n",
    "score, *_ = metric(\n",
    "    images=inputs, \n",
    "    return_overlap_mask=False, # Default \n",
    "    return_score_map=False, # Default \n",
    "    return_projections=False # Default \n",
    ")\n",
    "\n",
    "# Should be between 0.30 - 0.35\n",
    "print(score.mean().item())\n",
    "\n",
    "# Clear up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "used_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # in MB\n",
    "cached_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # in MB\n",
    "\n",
    "print(f\"Used memory: {used_memory:.2f} MB\")\n",
    "print(f\"Cached memory: {cached_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MEt3R with RAFT (Optical Flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from met3r import MEt3R\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Initialize MEt3R\n",
    "metric = MEt3R(\n",
    "    img_size=IMG_SIZE,\n",
    "    use_norm=True, \n",
    "    backbone=\"raft\",\n",
    "    feature_backbone=\"dino16\",\n",
    "    feature_backbone_weights=\"mhamilton723/FeatUp\",\n",
    "    upsampler=\"featup\",\n",
    "    distance=\"cosine\",\n",
    "    freeze=True, \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs of shape (batch, views, channels, height, width): views must be 2\n",
    "# RGB range must be in [-1, 1]\n",
    "# Reduce the batch size in case of CUDA OOM\n",
    "inputs = torch.randn((10, 2, 3, IMG_SIZE, IMG_SIZE)).cuda()\n",
    "inputs = inputs.clip(-1, 1)\n",
    "\n",
    "# Evaluate MEt3R\n",
    "score, *_ = metric(\n",
    "    images=inputs, \n",
    "    return_overlap_mask=False, # Default \n",
    "    return_score_map=False, # Default \n",
    "    return_projections=False # Default \n",
    ")\n",
    "\n",
    "# Should be between 0.17 - 0.18\n",
    "print(score.mean().item())\n",
    "\n",
    "# Clear up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MEt3R with VGGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGT_LIB_PATH /data1/usman/vision/met3r/vggt/vggt\n",
      "VGGT_REPO_PATH /data1/usman/vision/met3r/vggt\n",
      "inside MET3R init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data1/usman/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "/data1/usman/micromamba/env/envs/met3r/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /data1/usman/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Using backbone: vggt\n",
      "[DEBUG] Requested backbone: vggt\n",
      "VGGT model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/usman/micromamba/env/envs/met3r/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluate MEt3R\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m score, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overlap_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Default \u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_score_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Default \u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Default \u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Should be between 0.17 - 0.18\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(score\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/data1/usman/micromamba/env/envs/met3r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/usman/micromamba/env/envs/met3r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data1/usman/vision/met3r/met3r/met3r.py:412\u001b[0m, in \u001b[0;36mMEt3R.forward\u001b[0;34m(self, images, image_names, return_overlap_mask, return_score_map, return_projections)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvggt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvggt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_fn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_and_preprocess_images\n\u001b[0;32m--> 412\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_names\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(images\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    414\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_model(images)\n",
      "File \u001b[0;32m/data1/usman/vision/met3r/vggt/vggt/utils/load_fn.py:125\u001b[0m, in \u001b[0;36mload_and_preprocess_images\u001b[0;34m(image_path_list, mode)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mA quick start function to load and preprocess images for model input.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03mThis assumes the images should have the same shape for easier batching, but our model can also work well with different shapes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    - Dimensions are adjusted to be divisible by 14 for compatibility with model requirements\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Check for empty list\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_path_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least 1 image is required\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Validate mode\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from met3r import MEt3R\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Initialize MEt3R\n",
    "metric = MEt3R(\n",
    "    img_size=IMG_SIZE,\n",
    "    use_norm=True,\n",
    "    backbone=\"vggt\",\n",
    "    feature_backbone=\"dino16\",\n",
    "    feature_backbone_weights=\"mhamilton723/FeatUp\",\n",
    "    upsampler=\"featup\",\n",
    "    distance=\"cosine\",\n",
    "    freeze=True, \n",
    ").cuda()\n",
    "\n",
    "# Prepare inputs of shape (batch, views, channels, height, width): views must be 2\n",
    "# RGB range must be in [-1, 1]\n",
    "# Reduce the batch size in case of CUDA OOM\n",
    "inputs = torch.randn((10, 2, 3, IMG_SIZE, IMG_SIZE)).cuda()\n",
    "inputs = inputs.clip(-1, 1)\n",
    "\n",
    "# Evaluate MEt3R\n",
    "score, *_ = metric(\n",
    "    images=inputs, \n",
    "    return_overlap_mask=False, # Default \n",
    "    return_score_map=False, # Default \n",
    "    return_projections=False # Default \n",
    ")\n",
    "\n",
    "# Should be between 0.17 - 0.18\n",
    "print(score.mean().item())\n",
    "\n",
    "# Clear up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data1/usman/.cache/torch/hub/mhamilton723_FeatUp_main\n",
      "Using cache found in /data1/usman/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2551, 0.2609, 0.4051, 0.3508, 0.1960, 0.2474, 0.3170, 0.2876, 0.2531,\n",
      "        0.3401], device='cuda:0')\n",
      "0.29131874442100525\n",
      "[0.25509113073349, 0.2608541250228882, 0.4051186442375183, 0.35083553194999695, 0.19604437053203583, 0.24744966626167297, 0.31704163551330566, 0.28756314516067505, 0.25313231348991394, 0.3400569260120392]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from met3r import MEt3R\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Initialize MEt3R\n",
    "metric = MEt3R(\n",
    "    img_size=IMG_SIZE,\n",
    "    use_norm=False,\n",
    "    backbone=\"mast3r\",\n",
    "    feature_backbone=\"dino16\",\n",
    "    feature_backbone_weights=\"mhamilton723/FeatUp\",\n",
    "    upsampler=\"featup\",\n",
    "    distance=\"cosine\",\n",
    "    freeze=True, \n",
    ")\n",
    "metric = metric.cuda()\n",
    "\n",
    "# Prepare inputs of shape (batch, views, channels, height, width): views must be 2\n",
    "# RGB range must be in [-1, 1]\n",
    "# Reduce the batch size in case of CUDA OOM\n",
    "inputs = torch.randn((10, 2, 3, IMG_SIZE, IMG_SIZE)).cuda()\n",
    "inputs = inputs.clip(-1, 1)\n",
    "\n",
    "# Evaluate MEt3R\n",
    "score, *_ = metric(\n",
    "    images=inputs, \n",
    "    return_overlap_mask=False, # Default \n",
    "    return_score_map=False, # Default \n",
    "    return_projections=False # Default \n",
    ")\n",
    "\n",
    "# Should be between 0.17 - 0.18\n",
    "print(score)\n",
    "print(score.mean().item())\n",
    "print(score.cpu().numpy().tolist())\n",
    "\n",
    "# Clear up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path = \"/data1/usman/vision/data/re10k_subset/test/000000.torch\"\n",
    "data = torch.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'timestamps', 'cameras', 'images', 'key'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/watch?v=-aldZQifF2U'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['timestamps'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load image with alpha channel\n",
    "img_path = \"/data1/usman/vision/data/monocular_front/0a0bd11af4d8460f8a0f74ecf37023aa/front_frame0.png\"\n",
    "img = Image.open(img_path).convert(\"RGBA\")\n",
    "\n",
    "# Create white background image\n",
    "white_bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "\n",
    "# Paste original image onto white background using alpha channel as mask\n",
    "white_bg.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "\n",
    "# Save result (overwrite or use new name)\n",
    "white_bg.save(\"front_frame0_white.png\")\n",
    "img.save(\"front_frame0.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "video_path = \"/data1/usman/vision/data/test_video.mp4\"\n",
    "output_frames_dir = \"./output_frames\"\n",
    "output_video_path = \"./output_video.mp4\"\n",
    "start_time_sec = 30\n",
    "num_frames_to_read = 101\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(output_frames_dir, exist_ok=True)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open video file\")\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "start_frame = int(start_time_sec * fps)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Get frame size\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Read and save frames\n",
    "for i in range(num_frames_to_read):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Stopped early at frame {i} due to read failure.\")\n",
    "        break\n",
    "    # Save PNG\n",
    "    frame_filename = os.path.join(output_frames_dir, f\"frame_{i:04d}.png\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    # Write to output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots to: ./plots\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "json_path = \"experiments.json\"\n",
    "output_dir = \"./plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load JSON data\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize feature_backbone names\n",
    "name_map = {\n",
    "    'dino': 'DINO',\n",
    "    'dino16': 'DINO',\n",
    "    'dinov2': 'DINOv2',\n",
    "    'vit': 'ViT'\n",
    "}\n",
    "\n",
    "groups = {\"DINO\": [], \"DINOv2\": [], \"ViT\": []}\n",
    "\n",
    "# Grouping\n",
    "for exp in data:\n",
    "    key = exp[\"feature_backbone\"].lower()\n",
    "    group_name = name_map.get(key)\n",
    "    if group_name in groups:\n",
    "        label = f'{exp[\"backbone\"]}-{exp[\"experiment\"]}'\n",
    "        groups[group_name].append((label, exp[\"scores\"]))\n",
    "\n",
    "# Plotting\n",
    "for feature_backbone, series in groups.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for label, scores in series:\n",
    "        plt.plot(range(len(scores)), scores, label=label)\n",
    "\n",
    "    plt.title(f'{feature_backbone} MEt3R Scores Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('MEt3R Score')\n",
    "    plt.ylim(0, 0.6)  # <-- Fix y-axis upper limit to 0.3\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plot_path = os.path.join(output_dir, f'{feature_backbone}_met3r_scores.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved plots to: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "met3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
